\documentclass[a4paper,11pt]{kth-mag}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage[swedish,english]{babel}
\usepackage{modifications}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[toc]{glossaries}


\newglossaryentry{computer}
                 {
                   name=computer,
                   description={is a programmable machine that receives input,
                     stores and manipulates data, and provides
                     output in a useful format}
                 }


\newcommand{\todo}{ ... }

\newcommand{\ngram}{$n$-gram }
\newcommand{\ngrams}{$n$-grams }

\newcommand{\loremipsum}{
  {\color{lightgray}
  Fruit two greater fifth over every. In female fourth good wherein herb
  Waters yielding itself. Female greater. Hath in, second appear tree in.
  Him, it seasons. Upon. Good you're. Winged green. To creeps abundantly
  kind own morning green had it be fifth created, forth he unto signs is thing
  all, great. Place night Gathering upon were forth light deep. Abundantly.
  Kind air beginning his void seed it dry. Own and spirit may dry abundantly
  beast good forth. The fifth beginning. Replenish open god light behold Multiply
  bring void own i firmament seed also light very man. \gls{computer}

  }
}

\makeglossaries

\title{Something, something, something, thesis...}

\subtitle{Getting a degree is hard}
\foreigntitle{Getting a degree is hard in swedish too}
\author{Mattis Kancans Envall}
\date{June 2016}
\blurb{Master's Thesis at CSC\\Supervisor: Johan Boye\\Examiner: Viggo Kann}
\trita{TRITA xxx yyyy-nn}



\begin{document}
\frontmatter
\pagestyle{empty}
\removepagenumbers
\maketitle
\selectlanguage{english}
\begin{abstract}
  This is a skeleton for KTH theses. More documentation
  regarding the KTH thesis class file can be found in
  the package documentation.

\loremipsum
  
\end{abstract}
\clearpage
\begin{foreignabstract}{swedish}
  Denna fil ger ett avhandlingsskelett.
  Mer information om \LaTeX-mallen finns i
  dokumentationen till paketet.

\loremipsum

\end{foreignabstract}
\clearpage
\tableofcontents*

\glsaddall
\printglossaries

\mainmatter
\pagestyle{newchap}
\chapter{Introduction}
% poses a problem?
With increased internet usage, reviews online are becoming one of the most important resources when comparing businesses, services and products.
The increasing quantity of content makes for more reliable conclusions as more opinions can be taken into account, but it also proposes a problem,
since there is a limit to what human readers can process.

Computers are obviously faster and more capable to handle big quantities of data, which suggests potential for using computers as aid when
interpreting review-like content. This is studied under the name \emph{Sentiment Analysis}, and has been in very active research the last ten years
thanks to increases in available data and recent efforts to monetize it.

This degree project studies describes sentiment analysis with regard to mining and summarizing opinions in reviews, and provides a detailed study of one of the problems in this process.

%This general problem is called \emph{sentiment analysis}\cite{liu2012sentiment} and is widely considered a hard, non-trivial problem. 
%sentiment analysis is one of the most active fiels of research in NLP.

%The general problem of using computers and Natural Language Processing(NLP)-techniques to study opinions in text s called \emph{sentiment analysis}, and is one of the most active fields of study in NLP.

%In fact, recent trends of monetizing online content, not to mention the benefit of being able to 

\section{Sentiment analysis}
Sentiment analysis\cite{liu2012sentiment} is the general task of from raw text extracting and interpreting expressions with associated sentiments. This involves classifying orientation (and possibly intensity) of found sentiments, and identifying what entity, and possibly what aspect of that entity, is subject to the expressed sentiments. 

\subsection{Levels of sentiment analysis}
In general, sentiment analysis has been studied at three levels:

On the \emph{document level}, one overall sentiment of an entire document is identified. This is generally of limited use, as in most contexts, documents hold many opinions. Therefore most studies at the document level in some way handle conflicting opinions, although this may be done implicitly\cite[Chapter~3]{liu2012sentiment}. In document level sentiment analysis, the problem definition itself requires generalizations, which may invite to inaccurate over-simplifications.

\emph{Sentence level} sentiment analysis mitigates this risk of generalization; sentences are generally smaller than documents and thus less likely to hold conflicting opinions, so assuming there is at most one sentiment has less impact on results --- but the problem itself is not addressed. As an example, the sentence \emph{``Although the service is terrible, I still like this restaurant''} is arguably overall positive, but simply deeming it so excludes information which reduces quality of results and makes them susceptible to systematic errors.

%todo examples in table?
Ideally two opinions should be identified in the above sentence, one about \emph{service} and one about the reviewed entity in general. Sentiment analysis this fine grained is on the \emph{aspect level}. The goal is to find and classify individual opinions, which may  dealing with things like surjective opinions \emph{``food, service and ambience were all great''}, conjunctions \emph{``tasty but expensive''}, implicit references \emph{``Easy to fit in my pocket''} really implies \emph{``good size''}, and implicit references \emph{``I had pasta. It was great''}'. As such, sentiment analysis on the aspect level is commonly referred to as the most complicated, as it consists of several sub-problems\cite[chapter 1]{liu2012sentiment}.


\section{Sentiment Classification}
Sentiment classification can be either a classification problem or regression analysis problem\cite{liu2012sentiment}, depending on whether results are expected to be discrete class-labels or a continous measure of positivty. In this report the discrete classification definition will be used unless otherwise specified.

Various approaches to sentiment classification exist, where approaches can generally be categorized as either grammatical\cite{todo}, statistical\cite{todo}, or a combination of the two.

The following subsections apply to text classification in general, and thus also to sentiment classification.

\section{WordNet and word relations}
\subsection{Word senses encapsulate meaning}
The term \emph{word} can be rather ambigous or \emph{lemma}, 


\subsection{\ngram modeling}
\ngram modeling is a versitile and widely used method of modeling languages. Typically all $n$-length subsequences of a longer sequence are included in the model, and then frequencies of \ngrams are used in tasks. $n$-grams of length \emph{1,2,3...} are referred to as \emph{unigrams, bigrams} and \emph{trigrams}, respectively.

As an example, the \emph{bigrams} for the sentence \emph{``Languages are fun''} are:
\begin{quote}
  \vspace*{0.1cm}
  \centering
\emph{``\$ Languages''}, \emph{``Languages are''}, \emph{``are fun''}, \emph{``fun \$''},
\end{quote}
where \emph{\$} is a token meaning the start/end of sentence.

Although \ngram modelling can be applied to virtually any kind of sequence, this report henceforth will refer exclusively to \ngrams consisting of words.

If the probablity of a word's occurence in a sentence is known, then the probabilty of a sentence can be modeled using a \emph{unigram} model the following way thanks to the chain rule:
$$P(s) = \prod_{w_i \in s}P(w_i)$$, where $w_i$ is the $i$th word in the sentence, and $P$ is the probability of that word (usually basic )

Based on the Markov assumption, that the probability of a word's occurence only depend on previous word in the sentence, the above can be extended to \emph{bigrams}:

$$\sum_{w_i}P(w_i|w_{i-1})$$

\subsection{Smoothing}
A general problem when statiscially modelling based on existing data, is how the model should handle previously unseen entries.


\subsection{Backoff}

\subsection{Yelp's sentiment analyzer}

\chapter{Methodology}
\section{Evaluation}
A common way to evaluate information retrieval tasks is \emph{precision} and \emph{recall}. Together they evaluate a systems quality, by evaluationg the capacity to exclude irrelevant results, and include relevant results, respectively.

\subsection{Precision}
Precision is a measure of what fraction of retrieved results are considered relevant, in this work this means \todo.

It is defined as following:
$$Precision = \frac{\text {true positives}}{\text{true positives} + \text{false positives}}$$
%From this definition it should be clear that precision can by excluding uncertain items from retrieved results.

\subsection{Recall}
Similarly, recall is the fraction of relevant results that are retrieved:
$$Recall = \frac{\text {true positives}}{\text{true positives} + \text{false negatives}}$$

The reader may note from these definitions that more hesitance when including results would mean fewer false positives, but more false negatives, which implies that higher precision can be at the cost of lower recall.


\section{Aspect clustering}
\subsection{Using WordNet}
Based on the observation that there is usually some key words with lexiographical connection to the cluster, an alternative approach to traditional classification is proposed. This is motivated since classification models need complicated smoothing techinques and big training data sets to achieve good results.




\loremipsum
\loremipsum

\part{Important Results}

\chapter{First One}

\loremipsum

\section{Preliminaries}
\bibliographystyle{plain}
\bibliography{references}

\loremipsum
\loremipsum

\appendix
\addappheadtotoc
\chapter{RDF}\label{appA}

\begin{figure}[ht]
\begin{center}
And here is a figure
\caption{\small{Several statements describing the same resource.}}\label{RDF_4}
\end{center}
\end{figure}

that we refer to here: \ref{RDF_4}
\end{document}
